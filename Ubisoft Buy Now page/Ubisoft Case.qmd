---
title: "Ubisoft Interview Case"
format: 
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    toc-title: "Contents"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false

---

# Q1
What is the unit of analysis for the proposed A/B test? Explain.

```{r}
# import libraries
library(tidyverse)
library(readr)
library(dplyr)

# load data
df <- read_csv('https://raw.githubusercontent.com/jefftwebb/data/main/ubisoft_historical.csv')
summary(df)
head(df)
tail(df)

```
The unit of analysis is the entity whose behavior or outcome is being measured and analyzed to determine the impact of a change. Therefore, the unit of analysis here is either visitor (individual customer) or session.


# Q2
What is the baseline conversion rate for the For Honor game in the historical data?

```{r}
# calculate baseline conversion rate

baseline_cr <- df %>% 
  mutate(daily_cr = Conversions/Visitors) |> 
  summarise(baseline_cr = mean(daily_cr)) |>
  pull(baseline_cr)

baseline_cr
```

Baseline conversion rate = 0.05

# Q3
What is the required sample size in each group in order for the test to detect the specified MEI of 1% in conversions, assuming alpha of .05 and power of .8 in a one-tailed (directional) test?

```{r}
power.prop.test(p1 = baseline_cr, 
                p2 = baseline_cr + 0.01, 
                sig.level = .05, 
                power = .8, 
                alternative = "one.sided")

sample_size <- 6263 * 2

sample_size
```
We need 6263 observations for each group
=> Total sample size = 6263 * 2 = 12526

# Q4
Given the study parameters from the previous question—MEI, alpha and power—and the visitor counts in the historical data, how long will the test need to run? Discuss the assumptions you are making in estimating the test duration.

```{r}
avg_visitors <- df |>
  summarise(avg_visitors = mean(Visitors, na.rm = TRUE))|>
  pull(avg_visitors)

avg_visitors

duration <- sample_size/avg_visitors

duration
```

The test needs to run 24 days.

We assume no seasonal effect during these 3 months in historical data. For example, there is no date where the number of visitors grows significantly thanks to campaign/season/... and vice verse.

# Q5
In A/B testing false negatives can be more detrimental to a company than false positives. Primarily, they prevent the recognition and implementation of beneficial changes, resulting in missed opportunities for the company, and, in the long run, competitive disadvantage. False positives, by contrast, are an illusory success—mistaking random variation in the data as an improvement—and lead to changes that don’t make any difference. They waste resources.

Recalculate sample size and study duration with this distinction in mind, using different settings for alpha and power to allow for fewer false negatives and more false positives. Explain your choices.



```{r}
# Recalculate sample size and study duration for fewer false negatives and more false positives. 
power.prop.test(p1 = baseline_cr, 
                p2 = baseline_cr + 0.01, 
                sig.level = .03, 
                power = .9, 
                alternative = "one.sided")

new_sample_size <- 10131 * 2

new_sample_size

# Recalculate  study duration for fewer false negatives and more false positives. 

new_duration <- new_sample_size/avg_visitors

new_duration
```

I increase power to 0.9 and reduce alpha to 0.07. Thus, beta is 0.1 (lower than the original beta). Beta represents rates for false negatives while alpha represents that for false positives. This will allow for fewer false negatives and more false positives.

# Q6
Simulate visitor level data for the test, based on numbers from the historical data, and given the MEI and the test duration you calculated in Q4. The simulated data should resemble the actual data you would collect during the experiment. You can ignore weekly seasonality and marketing campaigns but the simulation should include a realistic number of visitors per day for the A and B groups for the duration of the experiment, as well as a realistic proportion of conversions. Each row should represent a unique customer-day combination. Use the simulated data to analyze the difference between A and B conversion rates statistically in order to show that your experimental design is sound. Report and explain your results. (Hint: conversion is a binary process that can be modeled as a binomial random variable.

```{r}
# set seed
set.seed(123)

# constants
total_visitors <- duration * avg_visitors
total_visitors
conversion_rate_control <- baseline_cr
conversion_rate_test <- baseline_cr + 0.01
conversion_rate_test

# Initialize a data frame with rows equal to # of total visitors
visitor_data <- data.frame(id = 1:total_visitors)

# Randomly assign each customer to either the control or test group
# Then simulate conversions based on expected test or control rates
visitor_data <- visitor_data |>
  mutate(group = sample(c("control", "test"), n(), replace = TRUE),
    conversion_prob = ifelse(group == "control", conversion_rate_control, conversion_rate_test),
    conversion = rbinom(n(), 1, conversion_prob))

head(visitor_data)

visitor_data |>
  group_by(group) |>
  summarize(rate = mean(conversion)) 

```
The treatment group has a 1.4% difference in conversion rate. We will check if it's statistically significant.

```{r}
table(factor(visitor_data$group, levels = c("test", "control")), 
      factor(visitor_data$conversion, levels = c(1, 0)))

# prop.test arguments:
# - x: vector or table giving a count of successes and failures
# - alternative: "greater" for a directional null hypothesis, meaning test (in
# the first row) > control (in second)

prop.test(x = table(factor(visitor_data$group, levels = c("test", "control")),
                    factor(visitor_data$conversion, levels = c(1, 0))), 
          alternative = "greater") 

```
With p-value = 0.0001874, this difference is statistically significant.

# Q7

Write a brief proposal for the experiment that includes discussion of:
- background and objectives
- the null and alternative hypotheses
- the test metric
- details on experimental design, including how customers will be allocated to the treatment conditions
- the sample size and experiment duration (and assumptions)
- analysis plan.

1. Project Overview
Ubisoft Entertainment is a French video game publisher headquartered in Paris, France. It is known for publishing games for several acclaimed video game franchises, including Assassin’s Creed, Just Dance, Far Cry, For Honor, and more.

This is a simulated school project based on a real A/B Test at Ubisoft, focusing on the For Honor game's Buy Now page. The redesign aims to simplify the buying process by reducing the need for scrolling and minimizing clicks. The goal is to increase customer conversions.

a. The Buy Now page before the testing phase:

User Journey:
- The user on the Buy Now page would first have to select the preferred version of the game from the five versions displayed on the screen.
- The user was then expected to scroll down and select the platform on which they would play.
- The final step was the Place Your Order button that took them to the checkout page on the Ubisoft store.

b. The New Test Variation:

- In the redesigned test layout, the section to choose edition, console, as well as the Order Now step were moved to the top of the left column, with an edition comparison.
- The new design eliminated scroll and improved the interaction time/rate on the Buy now page.

2. Test Design
a. Hypothesis:
- Null hypothesis: The new design has no impact on conversation rate.
- Alternative hypothesis: The new design increases the conversation rate by at least 1%.

b. Test metrics:
- Success metrics: conversion rate increases by 1%
- Minimum Effect of Interest (MEI) = 1


c. Experimental design 
- alpha = 0.05
- power = 0.8
- sample size = 6263 (per group) (allocation = 50%) 
- test duration = 24 days
- Note: The sampmle size and test duration are calculated based on historical data
- Assumptions: no seasonal effect during the test (campaign promotion, weekly seasonality, ...)
- Users will be randomly assigned to either the control group (existing design) or the treatment group (new design) to ensure unbiased comparisons.

3. Analysis plan:
- Step 1: Run A/B testing for the whole period (no change on the experimental design). During the testing period, follow the conversion to ensure the number is correct
- Step 2: Calculate conversion rates for each group and run a two-sample z-test for proportion to ensure statistical significance
- Step 3: Discuss the result with stakeholders and decide next step
- Note: Regardless of failed or succeessful test, document all insights and learning lessons for future reference

